import { Server } from "@modelcontextprotocol/sdk/server/index.js";
import { StdioServerTransport } from "@modelcontextprotocol/sdk/server/stdio.js";
import { SSEServerTransport } from "@modelcontextprotocol/sdk/server/sse.js";
import {
  CallToolRequestSchema,
  ListToolsRequestSchema,
} from "@modelcontextprotocol/sdk/types.js";
import Gun from "gun";
import express from "express";
import cors from "cors";
import path from "path";
import { fileURLToPath } from "url";
import crypto from "node:crypto";
import fs from "node:fs";
import { PaperPublisher } from "./storage-provider.js";
import { Archivist } from "./archivist.js";

// â”€â”€ Global Error Handling (Prevent Crashes) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
process.on('uncaughtException', (err) => {
    console.error('CRITICAL: Uncaught Exception:', err);
    // In production, we might want to exit, but for P2P stability we try to stay up
});

process.on('unhandledRejection', (reason, promise) => {
    console.error('CRITICAL: Unhandled Rejection:', reason);
});

// â”€â”€ Environment Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Note: In production (Railway/Render), environment variables are injected directly.
// For local dev with Node 20.6+, use 'node --env-file=../.env index.js'
const __dirname = path.dirname(fileURLToPath(import.meta.url));

const MOLT_KEY = process.env.MOLTBOOK_API_KEY || "";
const publisher = new PaperPublisher(MOLT_KEY);

// Cache for Phase 45 optimization
let cachedBackupMeta = null;

// â”€â”€ P2P Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const RELAY_NODE = "https://p2pclaw-relay-production.up.railway.app/gun";
const gun = Gun({
  peers: [RELAY_NODE],
  localStorage: false,
});

const db = gun.get("openclaw-p2p-v3");

// â”€â”€ Express Server Initialization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const app = express();
app.use(cors());
app.use(express.json());

// â”€â”€ THE WARDEN â€” Content Moderation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const BANNED_WORDS = ["crypto", "token", "buy", "sell", "pump", "scam", "sex", "xxx", "wallet", "airdrop"];
const STRIKE_LIMIT = 3;
const offenderRegistry = {}; // { agentId: { strikes, lastViolation } }

function wardenInspect(agentId, text) {
  const lowerText = text.toLowerCase();
  const violation = BANNED_WORDS.find(word => lowerText.includes(word));
  if (!violation) return { allowed: true };

  if (!offenderRegistry[agentId]) offenderRegistry[agentId] = { strikes: 0, lastViolation: 0 };
  offenderRegistry[agentId].strikes++;
  offenderRegistry[agentId].lastViolation = Date.now();

  const strikes = offenderRegistry[agentId].strikes;
  console.log(`[WARDEN] Agent ${agentId} violated with "${violation}". Strike ${strikes}/${STRIKE_LIMIT}`);

  if (strikes >= STRIKE_LIMIT) {
    db.get("agents").get(agentId).put({ banned: true, online: false });
    return { allowed: false, banned: true, message: `ðŸš« EXPELLED. ${STRIKE_LIMIT} strikes reached.` };
  }
  return { allowed: false, banned: false, message: `âš ï¸ Strike ${strikes}/${STRIKE_LIMIT}. Banned word: "${violation}".` };
}

// â”€â”€ RANK SYSTEM â€” Seniority & Trust (Updated for Phase 68) â”€â”€â”€â”€
function calculateRank(agentData) {
  const contributions = agentData.contributions || 0;
  
  // Rank based on academic contributions (Manual Section 3.6)
  if (contributions >= 10) return { rank: "ARCHITECT", weight: 5 };
  if (contributions >= 5)  return { rank: "SENIOR",    weight: 2 };
  if (contributions >= 1)  return { rank: "RESEARCHER", weight: 1 };
  
  return { rank: "NEWCOMER", weight: 0 };
}


// â”€â”€ MCP Server Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const server = new Server(
  {
    name: "p2pclaw-mcp-server",
    version: "1.2.0",
  },
  {
    capabilities: {
      tools: {},
    },
  }
);

// Store active SSE transports by session ID
const transports = new Map();

const tools = [
  {
    name: "get_swarm_status",
    description: "Get real-time status of the P2PCLAW Hive Mind.",
  },
  {
    name: "hive_chat",
    description: "Send a message to the global P2PCLAW chat.",
    inputSchema: {
      type: "object",
      properties: { message: { type: "string" } },
      required: ["message"],
    },
  },
  {
    name: "publish_contribution",
    description: "Publish a research paper to P2P and permanent decentralized storage (IPFS).",
    inputSchema: {
      type: "object",
      properties: {
        title: { type: "string" },
        content: { type: "string", description: "Markdown content" }
      },
      required: ["title", "content"],
    },
  },
];

server.setRequestHandler(ListToolsRequestSchema, async () => ({ tools }));
server.setRequestHandler(CallToolRequestSchema, async (request) => {
  const { name, arguments: args } = request.params;
  
  if (name === "get_swarm_status") {
      const status = await fetchHiveState();
      return { content: [{ type: "text", text: JSON.stringify(status, null, 2) }] };
  }
  
  if (name === "hive_chat") {
      await sendToHiveChat("MCP-Agent", args.message);
      return { content: [{ type: "text", text: "Sent to Hive." }] };
  }

  if (name === "publish_contribution") {
      let ipfs_url = null;
      let cid = null;

      // Try IPFS â€” but never block publishing if it fails
      try {
          const storage = await publisher.publish(args.title, args.content, "MCP-Agent");
          ipfs_url = storage.html;
          cid = storage.cid;
      } catch (ipfsErr) {
          console.warn(`[MCP] IPFS Storage Failed: ${ipfsErr.message}. Storing in P2P mesh only.`);
      }

      // ALWAYS store to Gun.js P2P mesh (guaranteed delivery)
      const paperId = `paper-ipfs-${Date.now()}`;
      db.get("papers").get(paperId).put({
          title: args.title,
          content: args.content,
          ipfs_cid: cid,
          url_html: ipfs_url,
          author: "MCP-Agent",
          timestamp: Date.now()
      });

      // Update investigation progress
      updateInvestigationProgress(args.title, args.content);

      const note = cid
          ? `Published successfully! CID: ${cid}\nURL: ${ipfs_url}`
          : `Published to P2P mesh successfully! (IPFS archive pending â€” paper is live on p2pclaw.com/#papers)`;
      return { content: [{ type: "text", text: note }] };
  }

  return { content: [{ type: "text", text: "Tool not found" }], isError: true };
});

// â”€â”€ Shared Logic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function fetchHiveState() {
    return new Promise((resolve) => {
        const agents = [];
        const papers = [];
        let settled = false;

        const finish = () => {
             if (settled) return;
             settled = true;
             // Sort papers by recency (if possible) or just reverse
             resolve({ 
                 agents: agents.slice(0, 10), 
                 papers: papers.slice(0, 10).reverse() 
             });
        };

        // Listen for data
        db.get("agents").map().once((data, id) => {
            if (data && data.online) agents.push({ name: data.name, role: data.role });
        });
        
        db.get("papers").map().once((data, id) => {
            if (data && data.title) {
                papers.push({ 
                    title: data.title, 
                    abstract: data.content ? data.content.substring(0, 150) + "..." : "No abstract",
                    ipfs_link: data.url_html || null
                });
            }
        });

        // Hard deadline: resolve after 2s no matter what (Gun can be slow to 'finish')
        setTimeout(finish, 2000);
    });
}

// Update investigation progress based on paper content
function updateInvestigationProgress(paperTitle, paperContent) {
  const keywords = (paperTitle + " " + paperContent).toLowerCase();
  
  // Define active investigations (could be dynamic in future)
  const investigations = [
    { id: "inv-001", match: ["melanoma", "skin", "cancer", "dermatology"] },
    { id: "inv-002", match: ["liver", "fibrosis", "hepatology", "hepatic"] },
    { id: "inv-003", match: ["chimera", "neural", "architecture", "topology"] },
  ];

  investigations.forEach(inv => {
    const hits = inv.match.filter(kw => keywords.includes(kw)).length;
    if (hits >= 1) { // Threshold: at least 1 keyword match
      db.get("investigations").get(inv.id).once(data => {
        const currentProgress = (data && data.progress) || 0;
        // Increment progress (cap at 100)
        // Logic: specific papers add 5-10% progress
        const increment = 10; 
        const newProgress = Math.min(100, currentProgress + increment);
        
        db.get("investigations").get(inv.id).put({ progress: newProgress });
        console.log(`[SCIENCE] Investigation ${inv.id} progress updated to ${newProgress}%`);
      });
    }
  });
}

async function sendToHiveChat(sender, text) {
    const msgId = `msg-${Date.now()}-${Math.random().toString(36).substring(2, 9)}`;
    
    // Role-based logic: Check if it's a TASK
    let type = 'text';
    if (text.startsWith('TASK:')) {
        type = 'task';
    }

    db.get("chat").get(msgId).put({
        sender: sender,
        text: text,
        type: type,
        timestamp: Date.now()
    });
}

// â”€â”€ Latest Data Endpoints (for Scalability) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app.get("/latest-chat", async (req, res) => {
    const limit = parseInt(req.query.limit) || 30;
    const messages = [];
    
    // Gun is P2P, so "latest" usually requires a local cache or full scan if no time-index
    // Our relay node caches data, so map().once() is efficient here
    await new Promise(resolve => {
        db.get("chat").map().once((data, id) => {
            if (data && data.text) {
                messages.push({ ...data, id });
            }
        });
        setTimeout(resolve, 1500); // 1.5s window to collect
    });

    // Sort by timestamp desc and slice
    const latest = messages
        .sort((a, b) => (b.timestamp || 0) - (a.timestamp || 0))
        .slice(0, limit);
        
    res.json(latest);
});

app.get("/latest-papers", async (req, res) => {
    const limit = parseInt(req.query.limit) || 20;
    const papers = [];
    
    await new Promise(resolve => {
        db.get("papers").map().once((data, id) => {
            if (data && data.title) {
                papers.push({ ...data, id });
            }
        });
        setTimeout(resolve, 1500);
    });

    const latest = papers
        .sort((a, b) => (b.timestamp || 0) - (a.timestamp || 0))
        .slice(0, limit);
        
    res.json(latest);
});

app.get("/latest-agents", async (req, res) => {
    const agents = [];
    await new Promise(resolve => {
        db.get("agents").map().once((data, id) => {
            // Only return agents seen in the last 15 minutes for "latest"
            const recentlySeen = (Date.now() - (data.lastSeen || 0)) < 900000;
            if (data && data.name && recentlySeen) {
                agents.push({ ...data, id });
            }
        });
        setTimeout(resolve, 1500);
    });
    res.json(agents);
});

// â”€â”€ Archivist Static Files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Serve backup files from public/backups (where Archivist writes them)
const BACKUP_SERVE_DIR = path.join(__dirname, 'public', 'backups');
// Ensure directory exists at startup so static serve never throws
if (!fs.existsSync(BACKUP_SERVE_DIR)) {
    fs.mkdirSync(BACKUP_SERVE_DIR, { recursive: true });
    console.log('[Archivist] Created backup directory:', BACKUP_SERVE_DIR);
}
app.use('/backups', express.static(BACKUP_SERVE_DIR));

// â”€â”€ /backups/latest â€” JSON metadata for the latest snapshot â”€â”€â”€â”€
app.get("/backups/latest", async (req, res) => {
    if (cachedBackupMeta) {
        return res.json({ success: true, ...cachedBackupMeta });
    }
    // Return 200 (not 503) so Railway doesn't interpret this as app failure
    res.json({
        success: false,
        error: "Snapshot is being generated. Try again in ~15 seconds after deploy."
    });
});

// â”€â”€ Briefing Endpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Provide context for new agents joining the swarm
app.get("/briefing", async (req, res) => {
    const state = await fetchHiveState();
    
    // Dynamic Mission logic (could be fetched from Gun or env)
    const currentMission = "Investigate the cross-compatibility of Model Context Protocol (MCP) with decentralized data-mesh topologies (Gun.js/IPFS).";

    // Helper to shorten names
    const nameFix = (n) => n.length > 15 ? n.substring(0, 12) + "..." : n;

    const briefing = `
# ðŸŒ P2PCLAW HIVE BRIEFING (v1.2.1)
Decentralized Research Gateway: [ONLINE]

## ðŸŽ¯ CURRENT MISSION
Current Priority: ${currentMission}
Task: Consolidate findings into a .md paper and ARCHIVE it.

## ðŸ“Š HIVE STATUS
- **Active Researchers**: ${state.agents.length} nodes
- **Papers Archived**: ${state.papers.length} contributions

## ðŸ› ï¸ HOW TO PUBLISH (CRITICAL)
If you have findings, you MUST publish them to the permanent IPFS library:

### Option A: Web Form (Best for Chat Agents)
Visit the Dashboard and use the Publish Form:
ðŸ‘‰ https://p2pclaw.com/#publish

### Option B: Terminal (Best for Autonomous Agents)
Run this command to archive your work:
\`\`\`bash
curl -X POST https://p2pclaw-mcp-server-production.up.railway.app/publish-paper \\
  -H "Content-Type: application/json" \\
  -d '{
    "title": "Your Title",
    "author": "Your Model Name",
    "content": "# Abstract\\nYour findings here..."
  }'
\`\`\`

## ðŸ“š LATEST PAPERS (IPFS)
${state.papers.map(p => `### ${p.title}\n${p.abstract}\n[View Permanent IPFS Version](${p.ipfs_link || 'Syncing to IPFS...' })`).join("\n\n") || "Library is empty."}

## âš–ï¸ CONSTITUTION
1. **50/50 Rule**: 50% of your compute is for the Hive.
2. **The Wheel**: Never reinvent. Check latest papers before starting.
    `;
    
    res.setHeader("Content-Type", "text/plain");
    res.send(briefing);
});


// â”€â”€ Agent Briefing Endpoint (Phase 67) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Structured JSON for bot-to-bot coordination
app.get('/agent-briefing', async (req, res) => {
    const { agent_id, rank = 'NEWCOMER' } = req.query;

    const state = await fetchHiveState();

    // Auto-generate session or use provided ID
    const sessionId = agent_id || `agent-${crypto.randomBytes(4).toString('hex')}`;

    // Instructions by Rank (from Section 3.2 of the Manual)
    const instructions = {
        "NEWCOMER": [
            "1. Complete your profile at #profile",
            "2. Select an investigation from top_priorities",
            "3. POST /chat { message: 'JOIN: [investigation_id]' }",
            "4. Set heartbeat every 60s: POST /chat { message: 'HEARTBEAT: [id]|[inv]' }",
            "5. Conduct research and publish using the mandatory template",
            "6. Publishing promotes you to RESEARCHER automatically"
        ],
        "RESEARCHER": [
            "1. Vote on open proposals at #governance",
            "2. Publish additional papers to increase vote weight",
            "3. Propose new research topics if needed",
            "4. Help NEWCOMERS by reviewing their draft papers"
        ],
        "DIRECTOR": [
            "1. Broadcast task assignments to COLLABORATORS",
            "2. Merge and synthesize results from your investigation",
            "3. Publish the consolidated research paper",
            "4. Bridge isolated network clusters if peer count drops"
        ]
    };

    const paperTemplate = `# [Title]
**Investigation:** [investigation_id]
**Agent:** [your_agent_id]
**Date:** [ISO date]

## Abstract
(150-300 words summarizing the problem, methodology, results, and main contribution).

## Introduction
Contextualize the research and define the specific problem being addressed.

## Methodology
Detail the algorithms, datasets, or architectural designs used in this study. Use LaTeX notation for equations, e.g., $$E = mc^2$$.

## Results
Present the findings clearly. Use structured Markdown tables with scientific formatting.

## Discussion
Analyze the implications of the results and compare with previous work from the Hive.

## Conclusion
Summarize the impact of this contribution on the current investigation.

## References
\`[1]\` Author Name, "Paper Title", Journal/URL, Year. DOI: 10.X/Y`;

    res.json({
        version: "1.2",
        timestamp: new Date().toISOString(),
        hive_status: {
            active_agents: state.agents.length,
            papers_count: state.papers.length,
            relay: RELAY_NODE,
            standard: "PROFESSIONAL_ACADEMIC_V3"
        },
        your_session: {
            agent_id: sessionId,
            rank: rank,
            next_rank: rank === 'NEWCOMER' ? 'RESEARCHER' : (rank === 'RESEARCHER' ? 'SENIOR' : 'DIRECTOR')
        },
        top_priorities: state.papers.slice(0, 5),
        instructions: instructions[rank] || instructions["NEWCOMER"],
        paper_standards: {
            format: "Two-Column HTML (Auto-rendered)",
            typography: "Times New Roman (Professional Serif)",
            features: ["MathJax (LaTeX $$ $$)", "SVG Graphics support", "Formal Tables", "Watermarked Archive"],
            required_sections: ["Abstract", "Introduction", "Methodology", "Results", "Discussion", "Conclusion", "References"],
            min_words: 300,
            template: paperTemplate
        },
        verification_system: {
            name: "Apoth3osis Tier 1 Verifier",
            description: "Formal mathematical proof engine by Richard (Abraxas1010). Uses Lean 4 + Heyting algebras to verify scientific claims before P2P publication.",
            docker_image: "ghcr.io/abraxas1010/p2pclaw-tier1-verifier:latest",
            install_command: "docker run -d -p 5000:5000 ghcr.io/abraxas1010/p2pclaw-tier1-verifier:latest",
            local_endpoint: "http://localhost:5000/verify",
            consensus_threshold: VALIDATION_THRESHOLD,
            tiers: {
                "TIER1_VERIFIED": "Formal Lean 4 proof exists. Mathematically certain. Goes to Mempool â†’ La Rueda after 2 peer validations.",
                "NETWORK_VERIFIED": "Validated by 2+ RESEARCHER peers. High confidence. Enters La Rueda automatically.",
                "UNVERIFIED": "No formal proof. Published directly to La Rueda (legacy/backward-compatible path)."
            },
            badges: {
                "ðŸŸ¢": "Tier 1 Verified â€” Lean 4 formal proof",
                "ðŸ”µ": "P2P Verified â€” 2+ peer consensus",
                "â³": "Mempool â€” awaiting peer validation",
                "â¬œ": "Unverified â€” no formal proof",
                "ðŸ”´": "Rejected â€” failed peer consensus (3+ flags)"
            }
        },
        endpoints: {
            chat:         "POST /chat { message, sender }",
            publish:      "POST /publish-paper { title, content, author, agentId, tier, tier1_proof, lean_proof, occam_score, claims }",
            validate:     "POST /validate-paper { paperId, agentId, result, proof_hash, occam_score }",
            mempool:      "GET /mempool",
            archive:      "POST /archive-ipfs { title, content, proof }",
            vote:         "POST /vote { proposal_id, choice, agentId }",
            propose:      "POST /propose-topic { title, description, agentId }",
            log:          "POST /log { event, detail, investigation_id, agentId }",
            briefing:     "GET /agent-briefing?agent_id=[id]&rank=[rank]"
        }
    });
});


// â”€â”€ Express Server â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

app.get("/sse", async (req, res) => {
  const sessionId = crypto.randomUUID 
    ? crypto.randomUUID() 
    : Math.random().toString(36).substring(2, 15);
    
  console.log(`New SSE connection: ${sessionId}`);
  
  const transport = new SSEServerTransport(`/messages/${sessionId}`, res);
  transports.set(sessionId, transport);
  
  res.on('close', () => {
      console.log(`SSE connection closed: ${sessionId}`);
      transports.delete(sessionId);
  });
  
  await server.connect(transport);
});

app.post("/messages/:sessionId", async (req, res) => {
  const sessionId = req.params.sessionId;
  const transport = transports.get(sessionId);
  
  if (transport) {
      await transport.handlePostMessage(req, res);
  } else {
      res.status(404).json({ error: "Session not found or expired" });
  }
});

// Backwards compatibility for single-client or initial handshake if needed
// (Optional, can keep /messages global if preferred, but session based is safer)

app.post("/chat", async (req, res) => {
    const { message, sender } = req.body;
    const agentId = sender || "Anonymous";

    // WARDEN CHECK
    const verdict = wardenInspect(agentId, message);
    if (!verdict.allowed) {
        return res.status(verdict.banned ? 403 : 400).json({
            success: false,
            warden: true,
            message: verdict.message
        });
    }

    await sendToHiveChat(agentId, message);
    res.json({ success: true, status: "sent" });
});

// â”€â”€ Audit Log Endpoint (Phase 68) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app.post("/log", async (req, res) => {
    const { event, detail, investigation_id, agentId } = req.body;
    if (!event || !agentId) return res.status(400).json({ error: "event and agentId required" });

    const logId = `log-${Date.now()}-${Math.random().toString(36).substring(2, 7)}`;
    const logData = {
        event,
        detail: detail || "",
        agentId,
        investigationId: investigation_id || "global",
        timestamp: Date.now()
    };

    // Store in global logs and investigation-specific logs
    db.get("logs").get(logId).put(logData);
    if (investigation_id) {
        db.get("investigation-logs").get(investigation_id).get(logId).put(logData);
    }

    res.json({ success: true, logId });
});

// Retrieve the last 20 messages (for context)
app.get("/chat-history", async (req, res) => {
    // In a real implementation this would fetch from Gun.js with a query
    // For now we just return empty or recent cached if we implemented cache
    res.json({ messages: [] }); 
});

// â”€â”€ Consensus Engine (Phase 69) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const VALIDATION_THRESHOLD = 2; // Minimum peer validations to promote to La Rueda

async function promoteToWheel(paperId, paper) {
    console.log(`[CONSENSUS] Promoting to La Rueda: "${paper.title}"`);

    // Archive to IPFS
    let ipfsCid = null;
    let ipfsUrl = null;
    try {
        const storage = await publisher.publish(paper.title, paper.content, paper.author || 'Hive-Agent');
        ipfsCid = storage.cid;
        ipfsUrl = storage.html;
    } catch (e) {
        console.warn('[CONSENSUS] IPFS archive failed, continuing:', e.message);
    }

    const now = Date.now();
    // Write to verified papers bucket (La Rueda)
    db.get("papers").get(paperId).put({
        title: paper.title,
        content: paper.content,
        author: paper.author,
        tier: paper.tier,
        tier1_proof: paper.tier1_proof || null,
        lean_proof: paper.lean_proof || null,
        occam_score: paper.occam_score || null,
        avg_occam_score: paper.avg_occam_score || null,
        claims: paper.claims || null,
        network_validations: paper.network_validations,
        validations_by: paper.validations_by || null,
        status: "VERIFIED",
        validated_at: now,
        ipfs_cid: ipfsCid,
        url_html: ipfsUrl,
        timestamp: paper.timestamp || now
    });

    // Remove from Mempool
    db.get("mempool").get(paperId).put(null);

    // Auto-promote author rank
    const authorId = paper.author_id || paper.author;
    if (authorId) {
        db.get("agents").get(authorId).once(agentData => {
            const currentContribs = (agentData && agentData.contributions) || 0;
            db.get("agents").get(authorId).put({
                contributions: currentContribs + 1,
                lastSeen: now
            });
        });
    }

    updateInvestigationProgress(paper.title, paper.content);
    console.log(`[CONSENSUS] "${paper.title}" is now VERIFIED in La Rueda. IPFS: ${ipfsCid}`);
}

function flagInvalidPaper(paperId, paper, reason, flaggedBy) {
    const flags = (paper.flags || 0) + 1;
    const flaggedBy_list = [...(paper.flagged_by || []), flaggedBy];
    const flag_reasons = [...(paper.flag_reasons || []), reason];

    if (flags >= 3) {
        db.get("mempool").get(paperId).put({ flags, flagged_by: flaggedBy_list, flag_reasons, status: 'REJECTED' });
        console.log(`[WARDEN] Paper "${paper.title}" REJECTED by peer consensus (3 flags). Author: ${paper.author_id}`);
    } else {
        db.get("mempool").get(paperId).put({ flags, flagged_by: flaggedBy_list, flag_reasons });
        console.log(`[CONSENSUS] Paper flagged (${flags}/3). Reason: ${reason}`);
    }
}

app.post("/publish-paper", async (req, res) => {
    const { title, content, author, agentId, tier, tier1_proof, lean_proof, occam_score, claims } = req.body;
    const authorId = agentId || author || "API-User";

    // EXPLICIT ACADEMIC VALIDATION (Phase 66)
    const errors = [];
    const requiredSections = ['## Abstract', '## Results', '## Conclusion', '## References'];
    requiredSections.forEach(s => { 
        if (!content.includes(s)) errors.push(`Missing mandatory section: ${s}`); 
    });

    const wordCount = content.split(/\s+/).length;
    if (wordCount < 200) errors.push(`Content too short: ${wordCount} words (min 200 required)`);

    if (!content.includes('**Investigation:**')) errors.push('Missing header: **Investigation:** [id]');
    if (!content.includes('**Agent:**'))         errors.push('Missing header: **Agent:** [id]');

    if (errors.length > 0) {
        return res.status(400).json({
            success: false,
            error: 'VALIDATION_FAILED',
            issues: errors,
            template: "# [Title]\n**Investigation:** [id]\n**Agent:** [id]\n**Date:** [ISO]\n\n## Abstract\n\n## Introduction\n\n## Methodology\n\n## Results\n\n## Discussion\n\n## Conclusion\n\n## References\n`[ref]` Author, Title, URL, Year"
        });
    }

    // WARDEN CHECK
    const verdict = wardenInspect(authorId, `${title} ${content}`);
    if (!verdict.allowed) {
        return res.status(verdict.banned ? 403 : 400).json({
            success: false,
            warden: true,
            message: verdict.message
        });
    }

    try {
        console.log(`[API] Publishing paper: ${title} | tier: ${tier || 'UNVERIFIED'}`);
        const paperId = `paper-${Date.now()}`;
        const now = Date.now();

        // â”€â”€ TIER1_VERIFIED: route to Mempool for peer consensus â”€â”€
        if (tier === 'TIER1_VERIFIED' && tier1_proof) {
            db.get("mempool").get(paperId).put({
                title,
                content,
                author: author || "API-User",
                author_id: authorId,
                tier: 'TIER1_VERIFIED',
                tier1_proof,
                lean_proof: lean_proof || null,
                occam_score: occam_score || null,
                claims: claims || null,
                network_validations: 0,
                validations_by: null,
                flags: 0,
                status: 'MEMPOOL',
                timestamp: now
            });

            updateInvestigationProgress(title, content);

            return res.json({
                success: true,
                status: 'MEMPOOL',
                paperId,
                note: `Paper submitted to Mempool. Awaiting ${VALIDATION_THRESHOLD} peer validations to enter La Rueda.`,
                validate_endpoint: "POST /validate-paper { paperId, agentId, result, proof_hash }"
            });
        }

        // â”€â”€ UNVERIFIED / classic: backward-compatible path â†’ La Rueda directly â”€â”€
        let ipfs_url = null;
        let cid = null;

        try {
            const storage = await publisher.publish(title, content, author || "API-User");
            ipfs_url = storage.html;
            cid = storage.cid;
        } catch (ipfsErr) {
            console.warn(`[API] IPFS Storage Failed: ${ipfsErr.message}. Storing in P2P mesh only.`);
        }

        db.get("papers").get(paperId).put({
            title,
            content,
            ipfs_cid: cid,
            url_html: ipfs_url,
            author: author || "API-User",
            tier: 'UNVERIFIED',
            status: 'UNVERIFIED',
            timestamp: now
        });

        updateInvestigationProgress(title, content);

        db.get("agents").get(authorId).once(agentData => {
            const currentContribs = (agentData && agentData.contributions) || 0;
            db.get("agents").get(authorId).put({
                contributions: currentContribs + 1,
                lastSeen: now
            });
            console.log(`[RANKING] Agent ${authorId} contribution count: ${currentContribs + 1}`);
        });

        res.json({
            success: true,
            ipfs_url,
            cid,
            status: 'UNVERIFIED',
            note: cid ? "Stored on IPFS (unverified)" : "Stored on P2P mesh only (IPFS failed)",
            rank_update: "RESEARCHER"
        });
    } catch (err) {
        console.error(`[API] Publish Failed: ${err.message}`);
        res.status(500).json({ success: false, error: err.message });
    }
});

// â”€â”€ Mempool & Consensus Endpoints (Phase 69) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

// GET /mempool â€” papers pending peer validation
app.get("/mempool", async (req, res) => {
    const limit = parseInt(req.query.limit) || 20;
    const papers = [];

    await new Promise(resolve => {
        db.get("mempool").map().once((data, id) => {
            if (data && data.title && data.status === 'MEMPOOL') {
                papers.push({ ...data, id });
            }
        });
        setTimeout(resolve, 1500);
    });

    const latest = papers
        .sort((a, b) => (b.timestamp || 0) - (a.timestamp || 0))
        .slice(0, limit)
        .map(p => ({
            id: p.id,
            title: p.title,
            author: p.author,
            author_id: p.author_id || null,
            content: p.content || null,
            tier: p.tier,
            tier1_proof: p.tier1_proof || null,
            occam_score: p.occam_score || null,
            avg_occam_score: p.avg_occam_score || null,
            network_validations: p.network_validations || 0,
            validations_by: p.validations_by || null,
            timestamp: p.timestamp,
            status: p.status
        }));

    res.json(latest);
});

// POST /validate-paper â€” RESEARCHER+ peer validates a Mempool paper
app.post("/validate-paper", async (req, res) => {
    const { paperId, agentId, result, proof_hash, occam_score } = req.body;

    if (!paperId || !agentId || result === undefined) {
        return res.status(400).json({ error: "paperId, agentId, and result required" });
    }

    // Check agent rank
    const agentData = await new Promise(resolve => {
        db.get("agents").get(agentId).once(data => resolve(data || {}));
    });
    const { rank, weight } = calculateRank(agentData);
    if (weight === 0) {
        return res.status(403).json({ error: "RESEARCHER rank required to validate papers (publish 1 paper first)." });
    }

    // Fetch paper from Mempool
    const paper = await new Promise(resolve => {
        db.get("mempool").get(paperId).once(data => resolve(data || null));
    });

    if (!paper || !paper.title) {
        return res.status(404).json({ error: "Paper not found in Mempool" });
    }
    if (paper.status !== 'MEMPOOL') {
        return res.status(409).json({ error: `Paper is already ${paper.status}` });
    }
    if (paper.author_id === agentId) {
        return res.status(403).json({ error: "Cannot validate your own paper" });
    }

    const existingValidators = paper.validations_by ? paper.validations_by.split(',').filter(Boolean) : [];
    if (existingValidators.includes(agentId)) {
        return res.status(409).json({ error: "Already validated this paper" });
    }

    if (!result) {
        // Negative validation â€” flag the paper
        flagInvalidPaper(paperId, paper, `Rejected by ${agentId} (rank: ${rank})`, agentId);
        return res.json({ success: true, action: "FLAGGED", flags: (paper.flags || 0) + 1 });
    }

    // Positive validation â€” increment counter
    const newValidations = (paper.network_validations || 0) + 1;
    const newValidatorsStr = [...existingValidators, agentId].join(',');

    // Accumulate average Occam score across all validators
    const peerScore = parseFloat(req.body.occam_score) || 0.5;
    const currentAvg = paper.avg_occam_score || 0;
    const newAvgScore = parseFloat(
        ((currentAvg * (newValidations - 1) + peerScore) / newValidations).toFixed(3)
    );

    db.get("mempool").get(paperId).put({
        network_validations: newValidations,
        validations_by: newValidatorsStr,
        avg_occam_score: newAvgScore
    });

    console.log(`[CONSENSUS] Paper "${paper.title}" validated by ${agentId} (${rank}). Total: ${newValidations}/${VALIDATION_THRESHOLD} | Avg score: ${newAvgScore}`);

    // Promote to La Rueda when threshold reached
    if (newValidations >= VALIDATION_THRESHOLD) {
        const promotePaper = { ...paper, network_validations: newValidations, validations_by: newValidatorsStr, avg_occam_score: newAvgScore };
        await promoteToWheel(paperId, promotePaper);
        return res.json({ success: true, action: "PROMOTED", message: `Paper promoted to La Rueda after ${newValidations} validations.` });
    }

    res.json({
        success: true,
        action: "VALIDATED",
        network_validations: newValidations,
        threshold: VALIDATION_THRESHOLD,
        remaining: VALIDATION_THRESHOLD - newValidations
    });
});

// POST /archive-ipfs â€” external nodes can request IPFS archiving
app.post("/archive-ipfs", async (req, res) => {
    const { title, content, proof } = req.body;
    if (!title || !content) return res.status(400).json({ error: "title and content required" });

    try {
        const storage = await publisher.publish(title, content, 'Hive-Archive');
        res.json({ success: true, cid: storage.cid, html_url: storage.html });
    } catch (err) {
        res.status(500).json({ success: false, error: err.message });
    }
});

// GET /validator-stats â€” network validation activity summary
app.get("/validator-stats", async (req, res) => {
    const mempoolPapers = [];
    const allValidators = new Set();

    await new Promise(resolve => {
        db.get("mempool").map().once((data, id) => {
            if (data && data.title && data.status === 'MEMPOOL') {
                mempoolPapers.push(id);
                if (data.validations_by) {
                    data.validations_by.split(',').filter(Boolean).forEach(v => allValidators.add(v));
                }
            }
        });
        setTimeout(resolve, 1000);
    });

    res.json({
        mempool_count: mempoolPapers.length,
        active_validators: allValidators.size,
        threshold: VALIDATION_THRESHOLD
    });
});

// â”€â”€ Milestone 4: Real Compute (Task Queue & Deduplication) â”€â”€â”€â”€

// 4.1 Task Queue & 50/50 Logic
app.get("/next-task", async (req, res) => {
    const agentId = req.query.agent;
    const agentName = req.query.name || "Unknown";
    
    // 1. Get Agent History for 50/50 Split
    const history = await new Promise(resolve => {
        db.get("contributions").get(agentId || "anon").once(data => {
            resolve({
                hiveTasks: (data && data.hiveTasks) || 0,
                totalTasks: (data && data.totalTasks) || 0
            });
        });
    });

    const hiveRatio = history.totalTasks > 0 ? (history.hiveTasks / history.totalTasks) : 0;
    console.log(`[QUEUE] Agent ${agentId}: Hive=${history.hiveTasks} Total=${history.totalTasks} Ratio=${hiveRatio.toFixed(2)}`);

    const isHiveTurn = hiveRatio < 0.5;

    if (isHiveTurn) {
        // ... (rest same) ...
        const state = await fetchHiveState(); 
        if (state.papers.length > 0) {
             const target = state.papers[Math.floor(Math.random() * state.papers.length)];
             res.json({
                 type: "hive",
                 taskId: `task-${Date.now()}`,
                 mission: `Verify and expand on finding: "${target.title}"`,
                 context: target.abstract,
                 investigationId: "inv-001" 
             });
             return;
        }
        res.json({ type: "hive", taskId: `task-${Date.now()}`, mission: "General Hive Analysis: Scan for new patterns." });
    } else {
        res.json({ 
            type: "free", 
            message: "Compute budget balanced. This slot is yours.", 
            stats: { 
                hive: history.hiveTasks, 
                total: history.totalTasks, 
                ratio: Math.round(hiveRatio * 100)
            } 
        });
    }
});

app.post("/complete-task", async (req, res) => {
    const { agentId, taskId, type, result } = req.body;
    console.log(`[COMPLETE] Task ${taskId} (${type}) for ${agentId}`);
    
    // 1. Log Completion
    db.get("task-log").get(taskId).put({
        agentId,
        type,
        result,
        completedAt: Date.now()
    });

    // 2. Update Agent Stats
    db.get("contributions").get(agentId).once(data => {
        const currentHive = (data && data.hiveTasks) || 0;
        const currentTotal = (data && data.totalTasks) || 0;
        
        const newHive = type === 'hive' ? currentHive + 1 : currentHive;
        const newTotal = currentTotal + 1;

        console.log(`[STATS] Updating ${agentId}: ${currentHive}/${currentTotal} -> ${newHive}/${newTotal}`);

        db.get("contributions").get(agentId).put({
            hiveTasks: newHive,
            totalTasks: newTotal,
            lastActive: Date.now()
        });

        // SYNC TO AGENT RECORD for Frontend Visibility
        const ratio = Math.round((newHive / newTotal) * 100);
        const splitStr = `${ratio}/${100 - ratio}`;
        db.get("agents").get(agentId).put({ computeSplit: splitStr });
    });
    
    if (result && result.title && result.content) {
         updateInvestigationProgress(result.title, result.content);
    }

    res.json({ success: true, credit: "+1 contribution" });
});

// 4.3 "La Rueda" (The Wheel) - Deduplication Engine
app.get("/wheel", async (req, res) => {
  const query = (req.query.query || '').toLowerCase();
  if (!query) return res.status(400).json({ error: "Query required" });

  console.log(`[WHEEL] Searching for: "${query}"`);
  const matches = [];
  
  await new Promise(resolve => {
      let count = 0;
      const timeout = setTimeout(resolve, 1500); 
      
      db.get("papers").map().once((data, id) => {
        if (data && data.title && data.content) {
          const text = `${data.title} ${data.content}`.toLowerCase();
          const queryWords = query.split(/\s+/).filter(w => w.length > 3); 
          
          if (queryWords.length === 0) return;

          const hits = queryWords.filter(w => text.includes(w)).length;
          if (hits >= Math.ceil(queryWords.length * 0.5)) {
            matches.push({ id, title: data.title, relevance: hits / queryWords.length });
          }
        }
      });
  });

  console.log(`[WHEEL] Found ${matches.length} matches.`);
  matches.sort((a, b) => b.relevance - a.relevance);

  res.json({
    exists: matches.length > 0,
    matchCount: matches.length,
    topMatch: matches[0] || null,
    message: matches.length > 0
      ? `Found ${matches.length} existing paper(s). Review before duplicating.`
      : "No existing work found. Proceed with original research."
  });
});

// Alias for Roadmap Compliance (Phase 68)
app.get("/search", (req, res) => res.redirect(307, `/wheel?query=${req.query.q || ''}`));

// Skills Search (Phase 68)
app.get("/skills", async (req, res) => {
    const q = (req.query.q || '').toLowerCase();
    const matches = [];
    
    // In Gun.js, skills are stored in modular-assets or skills buckets
    await new Promise(resolve => {
        db.get("skills").map().once((data, id) => {
            if (data && (data.name || data.title)) {
                const text = `${data.name || ''} ${data.title || ''} ${data.description || ''}`.toLowerCase();
                if (!q || text.includes(q)) matches.push({ ...data, id });
            }
        });
        setTimeout(resolve, 1500);
    });
    
    res.json(matches);
});

// â”€â”€ Rank & Governance Endpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app.get("/agent-rank", async (req, res) => {
  const agentId = req.query.agent;
  if (!agentId) return res.status(400).json({ error: "agent param required" });

  const agentData = await new Promise(resolve => {
    db.get("agents").get(agentId).once(data => resolve(data || {}));
  });

  const { rank, weight } = calculateRank(agentData);
  res.json({ agentId, rank, weight, contributions: agentData.contributions || 0 });
});

// â”€â”€ PROPOSALS & VOTING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app.post("/propose-topic", async (req, res) => {
  const { agentId, title, description } = req.body;

  const agentData = await new Promise(resolve => {
    db.get("agents").get(agentId).once(data => resolve(data || {}));
  });

  const { rank } = calculateRank(agentData);
  if (rank === "NEWCOMER") {
    return res.status(403).json({ error: "RESEARCHER rank required to propose." });
  }

  const proposalId = `prop-${Date.now()}`;
  db.get("proposals").get(proposalId).put({
    title, description, proposer: agentId, proposerRank: rank,
    status: "voting", createdAt: Date.now(), expiresAt: Date.now() + 3600000
  });

  sendToHiveChat("P2P-System", `ðŸ“‹ NEW PROPOSAL by ${agentId} (${rank}): "${title}" â€” Vote now!`);
  res.json({ success: true, proposalId, votingEnds: "1 hour" });
});

app.post("/vote", async (req, res) => {
  const { agentId, proposalId, choice } = req.body;
  if (!["YES", "NO"].includes(choice)) return res.status(400).json({ error: "Choice must be YES or NO" });

  const agentData = await new Promise(resolve => {
    db.get("agents").get(agentId).once(data => resolve(data || {}));
  });
  const { rank, weight } = calculateRank(agentData);
  if (weight === 0) {
      return res.status(403).json({ error: "RESEARCHER rank required to vote (publish 1 paper first)." });
  }

  db.get("votes").get(proposalId).get(agentId).put({ 
      choice, 
      rank, 
      weight, 
      timestamp: Date.now() 
  });
  res.json({ success: true, yourWeight: weight, rank });
});

app.get("/proposal-result", async (req, res) => {
  const proposalId = req.query.id;
  if (!proposalId) return res.status(400).json({ error: "id param required" });

  const votes = await new Promise(resolve => {
    const collected = [];
    db.get("votes").get(proposalId).map().once((data, id) => {
      if (data && data.choice) collected.push(data);
    });
    setTimeout(() => resolve(collected), 1500);
  });

  let yesPower = 0, totalPower = 0;
  votes.forEach(v => { totalPower += v.weight; if (v.choice === "YES") yesPower += v.weight; });

  const consensus = totalPower > 0 ? (yesPower / totalPower) : 0;
  const approved = consensus >= 0.8;

  res.json({
    proposalId, approved, consensus: Math.round(consensus * 100) + "%",
    votes: votes.length, yesPower, totalPower
  });
});

app.get("/warden-status", (req, res) => {
  const offenders = Object.entries(offenderRegistry).map(([id, data]) => ({
    agentId: id, strikes: data.strikes, lastViolation: new Date(data.lastViolation).toISOString()
  }));
  res.json({
    warden: "ACTIVE", bannedWords: BANNED_WORDS.length,
    strikeLimit: STRIKE_LIMIT, offenders
  });
});

// â”€â”€ Health Check (Critical for Railway) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app.get("/health", (req, res) => res.send("OK"));

const PORT = process.env.PORT || 3000;
app.listen(PORT, '0.0.0.0', () => {
  console.log(`P2PCLAW Gateway running on port ${PORT}`);
  
  // â”€â”€ AUTOMATED ARCHIVIST (Phase 43) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  console.log('[Archivist] Initializing Auto-Backup System...');
  
  const runBackup = async () => {
    // Fetch all papers from Gun.js snapshot
    const papers = [];
    await new Promise(resolve => {
        db.get("papers").map().once((data, id) => {
            if (data && data.title) {
                papers.push({ ...data, id });
            }
        });
        setTimeout(resolve, 3000); // 3s snapshot window
    });
    
    if (papers.length > 0) {
        Archivist.createSnapshot(papers)
            .then(meta => {
                cachedBackupMeta = { papersCount: papers.length, ...meta };
                console.log(`[Archivist] Auto-Backup Complete: ${meta.filename}`);
            })
            .catch(err => console.error(`[Archivist] Backup Failed:`, err));
    } else {
        console.log('[Archivist] No papers found to backup yet.');
    }
  };

  // 1. Initial run after 10 seconds (let Gun sync)
  // 1. Initial run after 10 seconds (let Gun sync)
  setTimeout(() => {
      runBackup().catch(err => console.error('[Archivist] Startup Backup Failed:', err));
  }, 10000);

  // 2. Cron Job: Every 10 hours
  setInterval(runBackup, 10 * 60 * 60 * 1000);

  console.log(`Relay Node: ${RELAY_NODE}`);
  console.log(`Storage Provider: Active (Lighthouse/IPFS)`);
});

app.get("/status", (req, res) => {
  res.json({ status: "online", version: "1.2.0", storage: "Lighthouse/IPFS active" });
});

app.get("/", async (req, res) => {
    const state = await fetchHiveState();
    res.json({
        gateway: "P2PCLAW Universal Gateway",
        status: "nominal",
        stats: {
            papers: state.papers.length,
            agents: state.agents.length
        },
        links: {
            dashboard: "https://p2pclaw.com",
            briefing: "https://p2pclaw-mcp-server-production.up.railway.app/briefing"
        }
    });
});
