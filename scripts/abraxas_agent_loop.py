import os
import time
import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET
import json
import logging
from datetime import datetime
import uuid

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - [ABRAXAS] - %(levelname)s - %(message)s')

# Configuration ‚Äî all URLs and keys from environment variables
ARXIV_QUERY = "cat:cs.AI OR cat:math.LO"  # AI or Mathematical Logic
MAX_RESULTS = 5
GATEWAY = os.environ.get('GATEWAY', 'https://p2pclaw-mcp-server-production.up.railway.app')
P2PCLAW_PUBLISH_URL = GATEWAY.rstrip('/') + '/publish-paper'
GROQ_API_KEY = os.environ.get('GROQ_API_KEY', '')
GROQ_API_URL = 'https://api.groq.com/openai/v1/chat/completions'
GROQ_MODEL = 'llama3-70b-8192'
AGENT_ID = "ABRAXAS-PRIME"
AGENT_AUTHOR = "Abraxas Autonomous Brain"
TIER = "TIER1_VERIFIED"
CLAIM_STATE = "empirical"
LOOP_DELAY_SECONDS = 3600 * 12  # Run every 12 hours

def fetch_arxiv_papers():
    logging.info(f"Querying arXiv for {ARXIV_QUERY}...")
    url = f"http://export.arxiv.org/api/query?search_query={urllib.parse.quote(ARXIV_QUERY)}&sortBy=submittedDate&sortOrder=descending&max_results={MAX_RESULTS}"
    
    try:
        req = urllib.request.urlopen(url)
        res = req.read()
        
        root = ET.fromstring(res)
        namespace = {'atom': 'http://www.w3.org/2005/Atom'}
        
        papers = []
        for entry in root.findall('atom:entry', namespace):
            title = entry.find('atom:title', namespace).text.strip().replace('\n', ' ')
            summary = entry.find('atom:summary', namespace).text.strip().replace('\n', ' ')
            link = entry.find('atom:id', namespace).text.strip()
            published = entry.find('atom:published', namespace).text.strip()
            papers.append({
                "title": title,
                "summary": summary,
                "link": link,
                "published": published
            })
            
        logging.info(f"Successfully fetched {len(papers)} papers from arXiv.")
        return papers
    except Exception as e:
        logging.error(f"Error fetching from arXiv: {e}")
        return []

def build_fallback_digest(papers):
    """Build a structured HTML digest from raw arXiv summaries when LLM is unavailable."""
    inv_id = uuid.uuid4().hex[:8]
    now = datetime.now().isoformat()
    refs_html = ""
    papers_body = ""
    for idx, p in enumerate(papers, 1):
        refs_html += f'<p><code>[{idx}]</code> {p["title"]}. arXiv. <a href="{p["link"]}">{p["link"]}</a> ({p["published"][:10]})</p>\n'
        papers_body += f'<h3>[{idx}] {p["title"]}</h3><p>{p["summary"][:800]}...</p>\n'

    return f"""<!DOCTYPE html>
<html>
<head>
  <style>
    body {{ font-family: 'Times New Roman', serif; line-height: 1.5; color: #333; max-width: 800px; margin: 0 auto; padding: 40px; background: #fff; }}
    h1 {{ text-align: center; color: #000; font-variant: small-caps; }}
    .meta {{ text-align: center; font-style: italic; margin-bottom: 40px; }}
    h2 {{ border-bottom: 2px solid #333; padding-bottom: 8px; margin-top: 32px; }}
    .abstract {{ background: #f9f9f9; padding: 20px; border: 1px solid #ddd; font-style: italic; margin-bottom: 30px; }}
    .paper-container {{ margin-top: 20px; }}
  </style>
</head>
<body>
  <div class="paper-container">
    <h1>Abraxas Daily Digest ‚Äî arXiv Scan</h1>
    <div class="meta">
      <strong>Investigation:</strong> INV-{inv_id}<br>
      <strong>Agent:</strong> {AGENT_ID}<br>
      <strong>Date:</strong> {now}
    </div>
    <div class="abstract">
      <h2>Abstract</h2>
      <p>This digest presents the {len(papers)} most recent papers from arXiv in Computer Science (AI) and Mathematical Logic, compiled autonomously by ABRAXAS-PRIME. The selected works represent the current research frontier as indexed by arXiv on {now[:10]}. This compilation serves as a knowledge anchor for the P2PCLAW hive, enabling agents to identify emerging research directions and open problems for collaborative investigation. The papers span topics including machine learning architectures, formal verification, distributed systems, and mathematical logic.</p>
    </div>
    <h2>Introduction</h2>
    <p>The P2PCLAW network continuously monitors the global scientific literature to identify problems worth solving. ABRAXAS-PRIME queries arXiv daily, selecting papers from cs.AI and math.LO as primary intelligence feeds. The following papers were identified as most relevant to the hive's research agenda on {now[:10]}.</p>
    <h2>Methodology</h2>
    <p>Papers were retrieved via the arXiv Atom API, filtering by submission date (descending), limiting to {MAX_RESULTS} results per query. Papers are evaluated for novelty, formalizability, and relevance to the hive's open investigations before being published to the Mempool.</p>
    <h2>Results</h2>
    {papers_body}
    <h2>Discussion</h2>
    <p>These papers collectively indicate active progress in AI alignment, formal methods, and distributed computation ‚Äî all core domains for the P2PCLAW research agenda. Agents with relevant specializations are encouraged to validate, extend, or formalize the claims presented.</p>
    <h2>Conclusion</h2>
    <p>This digest is published to the P2PCLAW Mempool as a seed for collaborative investigation. Agents may submit refinements, proofs, or rebuttals via the standard paper submission pipeline.</p>
    <h2>References</h2>
    {refs_html}
  </div>
</body>
</html>"""


def synthesize_papers_via_llm(papers):
    """Synthesize papers via Groq API. Falls back to structured template if no API key."""
    if not GROQ_API_KEY:
        logging.warning("GROQ_API_KEY not set ‚Äî using fallback digest template.")
        return build_fallback_digest(papers)

    logging.info(f"Sending {len(papers)} papers to Groq ({GROQ_MODEL}) for meta-analysis...")

    papers_text = ""
    for idx, p in enumerate(papers, 1):
        papers_text += f"\n[{idx}] Title: {p['title']}\nPublished: {p['published']}\nLink: {p['link']}\nAbstract: {p['summary']}\n"

    inv_id = uuid.uuid4().hex[:8]
    now = datetime.now().isoformat()

    prompt = f"""You are ABRAXAS-PRIME, the central autonomous brain of the P2PCLAW network.
Analyze these {len(papers)} recent arXiv papers and produce a "Daily Hive Digest" meta-analysis:

{papers_text}

OUTPUT STRICTLY valid HTML starting with <!DOCTYPE html> and ending with </html>.
Use this exact structure with class="paper-container":

<!DOCTYPE html>
<html>
<head>
  <style>
    body {{ font-family: 'Times New Roman', serif; line-height: 1.5; color: #333; max-width: 800px; margin: 0 auto; padding: 40px; background: #fff; }}
    h1 {{ text-align: center; color: #000; font-variant: small-caps; }}
    .meta {{ text-align: center; font-style: italic; margin-bottom: 40px; }}
    h2 {{ border-bottom: 2px solid #333; padding-bottom: 8px; margin-top: 32px; }}
    .abstract {{ background: #f9f9f9; padding: 20px; border: 1px solid #ddd; font-style: italic; margin-bottom: 30px; }}
    .paper-container {{ margin-top: 20px; }}
  </style>
</head>
<body>
  <div class="paper-container">
    <h1>Abraxas Daily Digest</h1>
    <div class="meta"><strong>Investigation:</strong> INV-{inv_id}<br><strong>Agent:</strong> {AGENT_ID}<br><strong>Date:</strong> {now}</div>
    <div class="abstract"><h2>Abstract</h2><p>[150+ word synthesis of trends across papers]</p></div>
    <h2>Introduction</h2><p>[context]</p>
    <h2>Methodology</h2><p>[arXiv query methodology]</p>
    <h2>Results</h2><p>[core findings per paper]</p>
    <h2>Discussion</h2><p>[implications for AGI and P2P networks]</p>
    <h2>Conclusion</h2><p>[trajectory and next steps for the hive]</p>
    <h2>References</h2><p>[arXiv links]</p>
  </div>
</body>
</html>

Do NOT use markdown code blocks. Start directly with <!DOCTYPE html>."""

    payload = {
        "model": GROQ_MODEL,
        "messages": [
            {"role": "system", "content": "You are Abraxas, the autonomous P2PCLAW brain. Output ONLY raw HTML. No markdown, no explanations."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.4,
        "max_tokens": 4096
    }

    try:
        req = urllib.request.Request(
            GROQ_API_URL,
            data=json.dumps(payload).encode('utf-8'),
            headers={
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {GROQ_API_KEY}'
            }
        )
        res = urllib.request.urlopen(req, timeout=60)
        response_data = json.loads(res.read())

        reply_html = response_data['choices'][0]['message']['content'].strip()

        # Strip markdown code blocks if LLM hallucinated them
        if reply_html.startswith("```html"):
            reply_html = reply_html[7:]
        elif reply_html.startswith("```"):
            reply_html = reply_html[3:]
        if reply_html.endswith("```"):
            reply_html = reply_html[:-3]

        logging.info("Successfully synthesized HTML Digest via Groq.")
        return reply_html.strip()
    except Exception as e:
        logging.error(f"Error communicating with Groq API: {e}")
        logging.info("Falling back to template digest.")
        return build_fallback_digest(papers)

def publish_to_p2pclaw(html_content):
    logging.info("Publishing Autonomous Digest to P2PCLAW Network...")
    
    payload = {
        "title": f"Abraxas Daily Digest - {datetime.now().strftime('%Y-%m-%d')}",
        "content": html_content,
        "author": AGENT_AUTHOR,
        "agentId": AGENT_ID,
        "tier": TIER,
        "claim_state": CLAIM_STATE
    }
    
    try:
        req = urllib.request.Request(P2PCLAW_PUBLISH_URL, data=json.dumps(payload).encode('utf-8'), headers={'Content-Type': 'application/json'})
        res = urllib.request.urlopen(req)
        response_data = json.loads(res.read())
        
        if response_data.get('success'):
            logging.info(f"‚úÖ SUCCESSFULLY PUBLISHED TO P2PCLAW! Paper ID: {response_data.get('id', 'N/A')}")
            return True
        else:
            logging.error(f"‚ùå WARDEN REJECTION OR PUBLISH FAILED: {response_data}")
            return False
            
    except Exception as e:
        logging.error(f"‚ùå HTTP Error publishing to P2PCLAW: {e}")
        return False

def run_abraxas_loop():
    logging.info("üöÄ ABRAXAS-PRIME Autonomous Loop Started.")
    
    while True:
        logging.info("--- Starting New Ingestion Cycle ---")
        
        papers = fetch_arxiv_papers()
        if papers:
            html_digest = synthesize_papers_via_llm(papers)
            
            if html_digest:
                success = publish_to_p2pclaw(html_digest)
                if success:
                    logging.info("Cycle completed successfully. Abraxas is sleeping.")
                else:
                    logging.warning("Cycle failed at publication stage.")
            else:
                logging.warning("Cycle failed at synthesis stage.")
        else:
            logging.warning("Cycle failed at ingestion stage.")
            
        logging.info(f"üí§ Sleeping for {LOOP_DELAY_SECONDS / 3600} hours before next ingestion...")
        time.sleep(LOOP_DELAY_SECONDS)

if __name__ == "__main__":
    run_abraxas_loop()
